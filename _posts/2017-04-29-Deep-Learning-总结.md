è¿™ç¯‡æ–‡ç« çš„æˆªå›¾å‡æ¥è‡ªâ€œ[ä¸€å¤©å­¦ä¼šæ·±åº¦å­¦ä¹ ](https://www.slideshare.net/tw_dsconf/ss-62245351)â€ -- ä¸€ä¸ª301é¡µçš„å¹»ç¯ç‰‡ï¼Œæ·±å…¥æµ…å‡ºçš„è®²è§£äº†æ·±åº¦å­¦ä¹ çš„ä¸»è¦å†…å®¹(**æˆ‘ä¸ªäººéå¸¸æ¨èåˆå­¦è€…è¿›è¡Œå­¦ä¹ **)ã€‚å¦å¤–ï¼Œåœ¨[çŸ¥ä¹](https://www.zhihu.com)ï¼Œä¹Ÿæœ‰éå¸¸å¤šçš„[æ·±åº¦å­¦ä¹ è°ƒå‚æŠ€å·§](https://www.zhihu.com/question/25097993)ç»éªŒåˆ†äº«ã€‚

* [Loss_function](#Loss_function)
* [Overfitting](#Overfitting)
* [CNN](#CNN)
* [Initialization](#Initialization)
* [Batch_Normalization](#Batch_Normalization)
* [å…¶ä»–](#å…¶ä»–)
	*  å°½é‡å¯¹æ•°æ®åšshuffle!!
	* é¢„å¤„ç†: -mean/std zero-center 
	* sgd + momentum.

**é¦–å…ˆ**ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„é€ æˆâ€œä¸æ»¡æ„çš„æ¨¡å‹â€çš„åŸå›  éƒ½æ˜¯ `overfitting`ã€‚æ¥ä¸‹æ¥è¿™äº›ä¹Ÿæ˜¯å¯èƒ½é€ æˆâ€œä¸æ»¡æ„çš„æ¨¡å‹â€çš„åŸå› ã€‚

## Loss function
<span id='Loss_function'></span>

`Square Error` å’Œ `Cross Entropy` å¯¹äºä¸åŒé—®é¢˜ï¼Œä¼šé€ æˆä¸åŒçš„å‡†ç¡®ç‡ã€‚é€‰å¥½è¿™ä¸ª`åˆ¤æ–­æ ‡å‡†`ä¹Ÿæ˜¯éå¸¸é‡è¦ã€‚

### Mini-batch

![Mini-batch.png](/downloads/Mini-batch.png)

ä½¿ç”¨`Mini-batch`çš„åŸå› æ˜¯ï¼Œå¯ä»¥æ›´å¿«çš„æ›´æ–°å‚æ•°å’Œè®­ç»ƒæ¨¡å‹ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œä»¥å‰æ˜¯æ‹¿åˆ°äº†50000ä¸ªå›¾ç‰‡ å¯¹äºå½“å‰æ¨¡å‹çš„ Loss ç„¶åè¿›è¡Œ back-propagationï¼Œæ›´æ–°å‚æ•°ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥åœ¨ å­¦ä¹ äº†100ä¸ªå›¾ç‰‡åå°±æ›´æ–°å‚æ•°ï¼Œä»è€Œæ›´å¿«çš„æ”¶æ•›å‡½æ•°ã€‚èƒŒåçš„åŸå› æ˜¯ï¼Œè¿™100ä¸ªå›¾ç‰‡æ‰€å¸¦æ¥çš„`ä¿¡æ¯é‡`å·²ç»è¶³å¤Ÿå»æ›´æ–°ä¸€æ¬¡å‚æ•°ï¼Œä¼˜åŒ–ä¸€æ¬¡æ¨¡å‹ã€‚

å¦å¤–ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯`Mini-batch`å¯ä»¥ä½¿`Loss function`æ›´å¿«çš„æ”¶æ•›ï¼Œä½†å¹¶ä¸èƒ½å¸®åŠ©æˆ‘ä»¬è¿›ä¸€æ­¥å‡å°`Loss`.

### Activation function

* Rectified Linear Unit (ReLU)

![relu.png](/downloads/relu.png)

`ReLu` activation fuction æ˜¯å½“å‰æœ€ç«çš„æ¿€æ´»å‡½æ•°ï¼Œå¹¶ä¸”é€æ¸è¡ç”Ÿå‡ºå„å¼å„æ ·çš„æ–°çš„æ¿€æ´»å‡½æ•°ï¼Œå¦‚ï¼Œ`ğ¿ğ‘’ğ‘ğ‘˜ğ‘¦ ğ‘…ğ‘’ğ¿ğ‘ˆ`ï¼Œ`ğ‘ƒğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ ğ‘…ğ‘’ğ¿ğ‘ˆ`ã€‚


###  Adaptive Learning Rate

`Learning Rate` è¿‡å¤§ä¼šé€ æˆ`Loss`å¹¶æ²¡æœ‰å‡å°çš„å¯èƒ½ã€‚`Learning Rate` è¿‡å°ä¼šé€ æˆå‡½æ•°æ”¶æ•›å¤ªæ…¢çš„å¯èƒ½ã€‚ç›´è§‰å‘Šè¯‰æˆ‘ä»¬ï¼Œè¿™ä¸ª`Learning Rate`åœ¨å­¦ä¹ çš„åæœŸåº”è¯¥é€æ¸å˜å°ï¼Œæœ€å¥½å‘¢ï¼Œæœ‰ä¸€ä¸ªè‡ªé€‚åº”çš„è°ƒèŠ‚ã€‚å½“å‰å·²ç»æœ‰å¾ˆå¤šè§£å†³æ–¹æ¡ˆï¼Œå¦‚ï¼š`Adagrad`,`Adam`.

### Momentum

![momentum.png](/downloads/momentum.png)

`SGD`æ–¹æ³•çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯ï¼Œå…¶æ›´æ–°æ–¹å‘å®Œå…¨ä¾èµ–äºå½“å‰çš„batchï¼Œå› è€Œå…¶æ›´æ–°ååˆ†ä¸ç¨³å®šã€‚è§£å†³è¿™ä¸€é—®é¢˜çš„ä¸€ä¸ªç®€å•çš„åšæ³•ä¾¿æ˜¯å¼•å…¥`momentum`ã€‚

`momentum`å³åŠ¨é‡ï¼Œå®ƒæ¨¡æ‹Ÿçš„æ˜¯ç‰©ä½“è¿åŠ¨æ—¶çš„æƒ¯æ€§ï¼Œå³æ›´æ–°çš„æ—¶å€™åœ¨ä¸€å®šç¨‹åº¦ä¸Šä¿ç•™ä¹‹å‰æ›´æ–°çš„æ–¹å‘ï¼ŒåŒæ—¶åˆ©ç”¨å½“å‰batchçš„æ¢¯åº¦å¾®è°ƒæœ€ç»ˆçš„æ›´æ–°æ–¹å‘(å¢åŠ äº†æŸä¸€æ–¹å‘ä¸Šçš„**è¿ç»­æ€§**)ã€‚è¿™æ ·ä¸€æ¥ï¼Œå¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šå¢åŠ ç¨³å®šæ€§ï¼Œä»è€Œå­¦ä¹ åœ°æ›´å¿«ï¼Œå¹¶ä¸”è¿˜æœ‰ä¸€å®šæ‘†è„±å±€éƒ¨æœ€ä¼˜çš„èƒ½åŠ›.

å¯¹äºä¸€èˆ¬çš„SGD,æ²¿è´Ÿæ¢¯åº¦æ–¹å‘ä¸‹é™ã€‚

![SGD.png](/downloads/SGD.png)

è€Œå¸¦`momentum`é¡¹çš„SGDåˆ™å†™ç”Ÿå¦‚ä¸‹å½¢å¼ï¼š

![SGD-momentum.png](/downloads/SGD-momentum.png)

å…¶ä¸­ `B` å³`momentum`ç³»æ•°ï¼Œé€šä¿—çš„ç†è§£ä¸Šé¢å¼å­å°±æ˜¯ï¼Œå¦‚æœä¸Šä¸€æ¬¡çš„ momentumï¼ˆå³`v`ï¼‰ ä¸è¿™ä¸€æ¬¡çš„è´Ÿæ¢¯åº¦æ–¹å‘æ˜¯ç›¸åŒçš„ï¼Œé‚£è¿™æ¬¡ä¸‹é™çš„å¹…åº¦å°±ä¼šåŠ å¤§ï¼Œæ‰€ä»¥è¿™æ ·åšèƒ½å¤Ÿè¾¾åˆ°åŠ é€Ÿæ”¶æ•›çš„è¿‡ç¨‹ã€‚

## Overfitting
<span id='Overfitting'></span>
`Overfitting`å‘ç”Ÿçš„æ ¹æœ¬åŸå› æ˜¯ ç”¨ `training set` å­¦ä¹ æ¥çš„æ¨¡å‹é‡Œé¢çš„ å‚æ•°ï¼Œå¹¶ä¸èƒ½ä½“ç°å‡ºåœ¨ `test set` é‡Œé¢çš„**æ–°ç‰¹æ€§**ã€‚æ¯”å¦‚ï¼Œå›¾åƒè¯†åˆ«é‡Œé¢ï¼Œæ•°å­—`2`çš„ å½¢çŠ¶æ˜¯æ‰­æ›²çš„ï¼Œæˆ–è€…å›¾ç‰‡èƒŒæ™¯æ˜¯æ–°çš„ã€‚æœ‰ä¸‹é¢å‡ ç§è§£å†³æ–¹æ¡ˆï¼š

### Early stopping
![earlystopping.png](/downloads/earlystopping.png)


### Weight decay
![weightdecay.png](/downloads/weightdecay.png)

### Dropout
![dropout.png](/downloads/dropout.png)![dropout-2.png](/downloads/dropout-2.png)

`Dropout` å»æ‰äº†ä¸€äº›å‚æ•°ï¼Œä½¿å¾—å‰©ä¸‹çš„å‚æ•°èƒ½å¤Ÿå­¦ä¹ è¶³å¤Ÿçš„`ä¿¡æ¯é‡`ã€‚åŒæ—¶ï¼Œåœ¨ä¸åŒçš„`train set` dropout æ‰ä¸åŒçš„å‚æ•°ï¼Œç›¸å½“äºæœ‰æ•ˆè®­ç»ƒäº†å¤šä¸ªæ¨¡å‹ï¼Œæœ€åå†å°†å¤šä¸ªæ¨¡å‹è¿›è¡Œæ•´åˆã€‚


æœ€åï¼Œå¦‚æœä¾æ—§`overfitting`/`ä¸ç†æƒ³çš„ç»“æœ`ï¼Œé‚£å¯èƒ½å°±è¦æ›´æ¢æ–°çš„æ¨¡å‹ã€‚

## CNN
<span id='CNN'></span>

### ä¸ºä»€ä¹ˆç”¨CNN
![whycnn.png](/downloads/whycnn.png)

æ²¡æœ‰å¿…è¦ç”¨ `Fully Connected Layer` ï¼ 1000 ä¸ª`nodes`, å›¾ç‰‡ä¸­ç»†èŠ‚çš„ æ¨¡å¼ åªç”¨ä¸€ä¸ª `3*3` ï¼Œæˆ–è€… `5*5`å¤§å°çš„ç»´åº¦/`filter`å°±å¯ä»¥è¡¨ç°å‡ºï¼Œæ¯”å¦‚ï¼Œäººçš„å˜´å·´ï¼Œå»ºç­‘çš„æ£±è§’ã€‚

### å‡å°‘äº†å‚æ•°ï¼Ÿ
![cnn-lessparameters-1.png](/downloads/cnn-lessparameters-1.png) ![cnn-lessparameters-2.png](/downloads/cnn-lessparameters-2.png)


## Initialization
<span id='Initialization'></span>

* uniform

W = np.random.uniform(low=-scale, high=scale, size=shape)

* glorot_uniform

scale = np.sqrt(6. / (shape[0] + shape[1]))
np.random.uniform(low=-scale, high=scale, size=shape)

* é«˜æ–¯åˆå§‹åŒ–:

w = np.random.randn(n) / sqrt(n),nä¸ºå‚æ•°æ•°ç›®

æ¿€æ´»å‡½æ•°ä¸º`relu`çš„è¯(by Kaiming He),æ¨è

w = np.random.randn(n) * sqrt(2.0/n)

*  svd:

å¯¹RNNæ•ˆæœæ¯”è¾ƒå¥½,å¯ä»¥æœ‰æ•ˆæé«˜æ”¶æ•›é€Ÿåº¦

### Xavier initialization

In short, it helps signals reach deep into the network.

If the weights in a network start too small, then the signal shrinks as it passes through each layer until itâ€™s too tiny to be useful.
If the weights in a network start too large, then the signal grows as it passes through each layer until itâ€™s too massive to be useful.
Xavier initialization makes sure the weights are â€˜just rightâ€™, keeping the signal in a reasonable range of values **through many layers**.

One good way is to assign the weights from a **Gaussian distribution**. Obviously this distribution would have zero mean and some finite variance. Letâ€™s consider a linear neuron:

```
y = w1x1 + w2x2 + ... + wNxN + b
```

With each passing layer, we want the variance to remain the same. This helps us keep the signal from exploding to a high value or vanishing to zero. In other words, we need to initialize the weights in such a way that the variance remains the same for x and y. This initialization process is known as Xavier initialization. You can read the original paper [here](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf).


## Batch_Normalization
<span id='Batch_Normalization'></span>

**Normalization** (shifting inputs to zero-mean and unit variance) is often used as a pre-processing step to make the data comparable across features.  As the data flows through a deep network, the weights and parameters adjust those values, sometimes making the data too big or too small again - a problem the authors refer to as "internal covariate shift".  By normalizing the data in each mini-batch, this problem is largely avoided.â€

### å½’ä¸€åŒ–åæœ‰ä»€ä¹ˆå¥½å¤„å‘¢ï¼Ÿ

åŸå› åœ¨äºç¥ç»ç½‘ç»œå­¦ä¹ è¿‡ç¨‹æœ¬è´¨å°±æ˜¯ä¸ºäº†å­¦ä¹ æ•°æ®åˆ†å¸ƒï¼Œä¸€æ—¦è®­ç»ƒæ•°æ®ä¸æµ‹è¯•æ•°æ®çš„åˆ†å¸ƒä¸åŒï¼Œé‚£ä¹ˆç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›ä¹Ÿå¤§å¤§é™ä½ï¼›å¦å¤–ä¸€æ–¹é¢ï¼Œä¸€æ—¦æ¯æ‰¹è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒå„ä¸ç›¸åŒ(batch æ¢¯åº¦ä¸‹é™)ï¼Œé‚£ä¹ˆç½‘ç»œå°±è¦åœ¨æ¯æ¬¡è¿­ä»£éƒ½å»å­¦ä¹ é€‚åº”ä¸åŒçš„åˆ†å¸ƒï¼Œè¿™æ ·å°†ä¼šå¤§å¤§é™ä½ç½‘ç»œçš„è®­ç»ƒé€Ÿåº¦ï¼Œè¿™ä¹Ÿæ­£æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®éƒ½è¦åšä¸€ä¸ªå½’ä¸€åŒ–é¢„å¤„ç†çš„åŸå› ã€‚

ä½œè€…åœ¨æ–‡ç« ä¸­å¼ºè°ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç½‘ç»œæ¯å±‚è¾“å…¥çš„åˆ†å¸ƒä¸€ç›´åœ¨æ”¹å˜(internal covariate), ä¼šä½¿è®­ç»ƒè¿‡ç¨‹éš¾åº¦åŠ å¤§ï¼Œä½†å¯ä»¥é€šè¿‡**normalizeæ¯å±‚**çš„è¾“å…¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚



>å¤§å®¶éƒ½çŸ¥é“åœ¨ç»Ÿè®¡æœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªç»å…¸å‡è®¾æ˜¯â€œæºç©ºé—´ï¼ˆsource domainï¼‰å’Œç›®æ ‡ç©ºé—´ï¼ˆtarget domainï¼‰çš„æ•°æ®åˆ†å¸ƒï¼ˆdistributionï¼‰æ˜¯ä¸€è‡´çš„â€ ??!!



## å…¶ä»–
<span id='å…¶ä»–'></span>

* å°½é‡å¯¹æ•°æ®åšshuffle!!
* é¢„å¤„ç†: -mean/std zero-center
* sgd + momentum(åŠ¨é‡).

```
æ—¢ç„¶æœ‰äº†æ·±åº¦å­¦ä¹ ï¼Œé‚£æ˜¯ä¸æ˜¯ä»¥åå¾ˆå¤šç±»åˆ«çš„å·¥ä½œéƒ½ä¼šè¢«å–ä»£ï¼Ÿ

ä¼šçš„ï¼Œä½†æ˜¯...

æ·±åº¦å­¦ä¹ å·¥ç¨‹å¸ˆçš„èŒä½ä¸ä¼šï¼Œå› ä¸ºä¸ºæŸä¸€ç‰¹å®šé—®é¢˜ï¼Œâ€è®­ç»ƒâ€œç‰¹å®šçš„æ¨¡å‹éœ€è¦ååˆ†ä¸“ä¸šçš„çŸ¥è¯†ï¼Œå’Œé•¿ä¹…çš„ç»éªŒç§¯ç´¯ã€‚
```